{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2511a246",
   "metadata": {},
   "outputs": [],
   "source": [
    "#! pip install pycaret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3269a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import graphistry\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pycaret\n",
    "\n",
    "from sklearn.cluster import DBSCAN\n",
    "from collections import Counter\n",
    "\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "from joblib import load, dump\n",
    "\n",
    "import spacy\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8a1dc1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaae24ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "graphistry.register(api=3, protocol=\"https\", server=\"hub.graphistry.com\", username=\"...\", password=\"...\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f71bd0cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = graphistry.bind()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71e8dd36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv('~/Downloads/instances.csv')\n",
    "# names = df.columns\n",
    "# df2 = pd.read_csv('~/Downloads/instances 2.csv', names=names)\n",
    "# df3 = pd.read_csv('~/Downloads/instances 3.csv', names=names)\n",
    "# df4 = pd.read_csv('~/Downloads/instances 4.csv', names=names)\n",
    "# df5 = pd.read_csv('~/Downloads/instances 5.csv', names=names)\n",
    "# df6 = pd.read_csv('~/Downloads/instances 6.csv', names=names)\n",
    "# mdf = pd.concat([df, df2, df3, df4, df5, df6], ignore_index=True)\n",
    "# mdf = mdf.drop_duplicates(ignore_index=True)\n",
    "# mdf['n'] = range(len(mdf))\n",
    "# mdf.to_csv('data/darkweb.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82d2b3a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we have a total df we can load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd5987e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/darkweb.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c84a4ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.fillna('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cdf8530",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['published_date'] = pd.to_datetime(df['published_date'], errors='coerce')\n",
    "df = df.sort_values(by='published_date', ascending=True, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f6e6a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b279826",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_date = df.groupby('published_date').size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba0f80bc",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_date.plot(figsize=(15,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00880e3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cols = ['event_type', 'fragment', 'document_title', 'document_authors']  # the rest are noise for featurization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "047f6638",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df_cols] = df[df_cols].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80813c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is not needed, so the following can be skipped\n",
    "process = False\n",
    "\n",
    "if process:\n",
    "    g2 = g.nodes(df, 'n').featurize(use_columns=df_cols)\n",
    "    # now save the features so we don't have to reprocess \n",
    "    X = g2.node_features\n",
    "    dump(X, 'data/darkweb_features.pickle')\n",
    "else:\n",
    "    X = load('data/darkweb_features.pickle')\n",
    "    g2 = g.nodes(df, 'n')\n",
    "    g2.node_features = X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f09c1344",
   "metadata": {},
   "outputs": [],
   "source": [
    "g3 = g2.umap(scale=.5, n_neighbors=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59a159ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "g3.nodes(df[df_cols+['n']], 'n').plot() # what a mess, but nice clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "918a5ec5",
   "metadata": {},
   "source": [
    "# Let's cluster and see how well dimensional reduction has done to group similar records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ca54a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "emb = g3.node_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f75da80",
   "metadata": {},
   "outputs": [],
   "source": [
    "clustering = DBSCAN(eps=3, min_samples=2).fit(emb)\n",
    "labels = clustering.labels_\n",
    "cnt = Counter(labels).most_common()\n",
    "cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b8da701",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(cnt) # lots of clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd804cc2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cdf = df[labels==3]  #makes sense\n",
    "cdf[df_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f15ab042",
   "metadata": {},
   "outputs": [],
   "source": [
    "cdf.resample('Q').size()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2c1094d",
   "metadata": {},
   "source": [
    "# Use SpaCy to see if we can extract some useful info and de-noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8536a9ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "OBJECT_DEPS = {\"dobj\", \"dative\", \"attr\", \"oprd\"}\n",
    "SUBJECT_DEPS = {\"nsubj\", \"nsubjpass\", \"csubj\", \"agent\", \"expl\"}\n",
    "# tags that define wether the word is wh-\n",
    "WH_WORDS = {\"WP\", \"WP$\", \"WRB\"}\n",
    "\n",
    "# extract the subject, object and verb from the input\n",
    "def get_svo_lemmas(doc):\n",
    "    \"\"\"Get Subject Verb Object Triples\"\"\"\n",
    "    sub = []\n",
    "    at = []\n",
    "    ve = []\n",
    "    for token in doc:\n",
    "        # is this a verb?\n",
    "        if token.pos_ == \"VERB\":\n",
    "            ve.append(token.lemma_)\n",
    "        # is this the object?\n",
    "        if token.dep_ in OBJECT_DEPS or token.head.dep_ in OBJECT_DEPS:\n",
    "            at.append(token.lemma_)\n",
    "        # is this the subject?\n",
    "        if token.dep_ in SUBJECT_DEPS or token.head.dep_ in SUBJECT_DEPS:\n",
    "            sub.append(token.lemma_)\n",
    "    return [k for k in zip(sub, ve, at)]\n",
    "\n",
    "def extract_named_ents(text):\n",
    "    \"\"\"Extract named entities\n",
    "    \"\"\"\n",
    "    return [(ent.text, ent.label_) for ent in nlp(text).ents]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e554d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = df.fragment.sample(1).values[0]\n",
    "doc = nlp(doc)\n",
    "get_svo_lemmas(doc) #meh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcb993cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_named_ents(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbd736f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "good_cols = ['n', 'fragment', 'document_title', 'document_authors', 'event_type', 'document_id']\n",
    "good_cols2 = ['n', 'fragment', 'document_title']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af8a0dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# these aren't useful in this format\n",
    "#ents = df.fragment.apply(lambda x: extract_named_ents(nlp(str(x))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "791429d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ents.values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "322df582",
   "metadata": {},
   "outputs": [],
   "source": [
    "#nents = [(k, v) for k, v in enumerate(ents.values) if v != list([])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27578282",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#nents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "159de7da",
   "metadata": {},
   "source": [
    "# Let's analyze using ngram analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a298ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "doclength = df.document_title.apply(lambda x: len(x.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27ec62ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['title_length'] = doclength"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acd8f2b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "doclength.plot(kind='hist', log=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73d217ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['fragment_length'] = df.fragment.apply(lambda x: len(str(x).split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1983369a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.fragment_length.hist(log=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12d0016f",
   "metadata": {},
   "source": [
    "# Let's prune to documents that have minimum length \n",
    "## Smaller sample for prototyping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14ef1a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "bdf = df[(df.fragment_length>5) & (df.title_length>5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a09d2a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "bdf['n'] = range(len(bdf)) # bdf will be what we use for most of subsequent analysis until we pull in other sources"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca3a5634",
   "metadata": {},
   "source": [
    "# let's find a word to word score\n",
    "## This will be useful to find, quickly, what sellers are involved in over all the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04421f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "# wget https://www.sketchengine.eu/english-word-list/\n",
    "reader = csv.reader(open('data/english-word-list-total.csv', 'r'))\n",
    "data = [row for row in reader]\n",
    "remove_words = [data[k][0].split(';')[1] for k in range(len(data))]\n",
    "remove_words = remove_words[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c0bf99a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's encode as ngrams with usernames too\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "cvect = CountVectorizer(min_df=3, max_df=0.35, ngram_range=(1,3), stop_words=remove_words)\n",
    "\n",
    "# make the matrix with seller included\n",
    "M = np.array(cvect.fit_transform(\n",
    "    bdf.fragment.astype(str).values + \n",
    "    ' ' + bdf.document_title.astype(str).values +\n",
    "    ' ' + bdf.document_authors.values).todense())\n",
    "\n",
    "top_sims = 10\n",
    "coldict = {k:v for v, k in cvect.vocabulary_.items()}\n",
    "ww = np.cov(M.T)\n",
    "w2w = [[coldict[k] for k in row.argsort()[::-1][:top_sims]] for row in ww]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b8ff149",
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_in_key(word, wdict):\n",
    "    indices = []\n",
    "    wordlist = []\n",
    "    for w, indx in wdict.items():\n",
    "        if word in w:\n",
    "            indices.append(indx)\n",
    "            wordlist.append(w)\n",
    "    return wordlist, indices\n",
    "\n",
    "\n",
    "def get_top_correlated_words(word, verbose=False):\n",
    "    wordlist, indices = word_in_key(word.lower(), cvect.vocabulary_)\n",
    "    seen = set()\n",
    "    bests = []\n",
    "    for w, i in zip(wordlist, indices):\n",
    "        if w not in seen:\n",
    "            #print(f'{word}: {w} -> {w2w[i]}')\n",
    "            bests+= w2w[i]\n",
    "        else:\n",
    "            seen.add(w)\n",
    "    bests = sorted(np.unique(bests))\n",
    "    print(f'{word} -> {bests}') if verbose else None\n",
    "    return bests\n",
    "\n",
    "def get_random_word():\n",
    "    return np.random.choice(list(cvect.vocabulary_))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38d49c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If we want to see how well entities correlate via ngrams -- not as good as g3.umap().plot()\n",
    "# cc = np.cov(M)\n",
    "# rows, cols = cc.nonzero()\n",
    "# edges = pd.DataFrame({'a': rows, 'b':cols, 'weight': cc[rows, cols]})\n",
    "# edges_pruned = prune_weighted_edges_df(edges, scale=8)\n",
    "# g = graphistry.nodes(bdf, 'n').edges(edges_pruned, 'a', 'b')\n",
    "# g.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1944a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# user 'word cloud' \n",
    "r=get_top_correlated_words('DopeValley', verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec633e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_likely_drugs_or_region(username):\n",
    "    \"\"\" Gets likely drugs/items/region for seller names\"\"\"\n",
    "    r=get_top_correlated_words(username)\n",
    "    drugs = Counter()\n",
    "    for a in r:\n",
    "        res = extract_named_ents(nlp(str(a)))\n",
    "        if len(res):\n",
    "            for k in res:\n",
    "                if len(k)==2:\n",
    "                    aa, b = k\n",
    "                    if b in ['GPE', 'ORG', 'NP', 'PERSON']:\n",
    "                        res = aa.split()[0]\n",
    "                        if res.lower() != username.lower():\n",
    "                            drugs.update([res])\n",
    "    if len(drugs):\n",
    "        print(f'{username} is likely selling (item/drug/region) -- ')\n",
    "        print('-'*30)\n",
    "        for n, c in drugs.most_common():\n",
    "            print(f'\\t{n.upper()}  ->  score {c/len(drugs):.2f}')\n",
    "        print()\n",
    "    return drugs\n",
    "\n",
    "c=get_likely_drugs_or_region('DopeValley')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98d2b87c",
   "metadata": {},
   "outputs": [],
   "source": [
    "bdf['seller'] = bdf.document_authors.apply(lambda x: x.split()[0].replace('[', '').replace(']', ''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eac80b32",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "bdf.groupby('seller').agg('mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc8a5a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# instead of username, put in anything...\n",
    "r=get_likely_drugs_or_region('cocaine')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9914bf7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all sellers and see what they might be selling\n",
    "users = Counter(bdf.seller).most_common()\n",
    "resses = []\n",
    "for user, count in users:\n",
    "    r = get_likely_drugs_or_region(user)\n",
    "    resses.append(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d567a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import DictVectorizer\n",
    "dvect = DictVectorizer()\n",
    "\n",
    "dm = np.array(dvect.fit_transform(resses).todense())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1149d358",
   "metadata": {},
   "outputs": [],
   "source": [
    "dm  # users x NER tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a73c3bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "doldict = {k:v for v, k in dvect.vocabulary_.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "095dd326",
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = dm.sum(0).argsort()[::-1]\n",
    "drug_cols = [doldict[k] for k in indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "756b85c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,10))\n",
    "plt.imshow(dm, aspect='auto')\n",
    "\n",
    "topN = 20\n",
    "plt.xticks(indices[:topN], np.array(drug_cols)[indices[:topN]], rotation=70)\n",
    "\n",
    "\n",
    "plt.yticks(np.arange(0, len(dm), 5), np.array([u for u, c in users])[::5], rotation=40)\n",
    "print('Users by top (items/drugs/locations)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1ba2445",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,10))\n",
    "plt.plot(dm.sum(0))\n",
    "topN = 20\n",
    "plt.xticks(indices[:topN], np.array(drug_cols)[indices[:topN]], rotation=70)\n",
    "\n",
    "print('Abundance over time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2306e98c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,12))\n",
    "plt.imshow(np.cov(dm.T))\n",
    "plt.xticks(indices[:topN], np.array(drug_cols)[indices[:topN]], rotation=70)\n",
    "plt.yticks(indices[:topN], np.array(drug_cols)[indices[:topN]], rotation=30)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4427186e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cc = np.cov(dm.T)\n",
    "sims = [[doldict[k] for k in row.argsort()[::-1][1:5]] for row in cc]\n",
    "sims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a791865a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# some examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "648df6fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "sims = get_top_correlated_words(get_random_word(), verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2515e8de",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sims = get_top_correlated_words(get_random_word(), verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a332a6d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sims = get_top_correlated_words(get_random_word(), verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e50cbd7c",
   "metadata": {},
   "source": [
    "# Now let's see words per seller (time ordered) [Not Interesting]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b146c5ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "groups = [bdf[bdf.seller==user] for user, c in users]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dda0f753",
   "metadata": {},
   "outputs": [],
   "source": [
    "groups[1].fragment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4a291d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0ef715f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mats = []\n",
    "for i, gdf in enumerate(groups):\n",
    "    m = np.array(cvect.transform(gdf.fragment.astype(str).values + \n",
    "    ' ' + gdf.document_title.astype(str).values +\n",
    "    ' ' + gdf.document_authors.values).todense())\n",
    "    mats.append(m)\n",
    "    print(users[i], m.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d8be3ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "[coldict[k] for k in mats[0].sum(0).argsort()[::-1][:topN]] # better using above funcs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ee77ebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "[coldict[k] for k in mats[1].sum(0).argsort()[::-1][:topN]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64b129f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sims = get_top_correlated_words(get_random_word())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46542dc0",
   "metadata": {},
   "source": [
    "# Featurize and Cluster in bdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8837177",
   "metadata": {},
   "outputs": [],
   "source": [
    "good_cols3 = ['fragment', 'document_title', 'document_authors']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "980b49ea",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "g2 = graphistry.nodes(bdf, 'n').featurize(use_columns=good_cols3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84b21a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "g3 = g2.umap(scale=1, n_neighbors=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "839aedfe",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "g3.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb3ec48e",
   "metadata": {},
   "outputs": [],
   "source": [
    "bdf['time'] = pd.to_datetime(bdf['published_date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e93f017b",
   "metadata": {},
   "outputs": [],
   "source": [
    "bdf.resample('W', on='time')['seller'].count().plot() #boo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52959df3",
   "metadata": {},
   "source": [
    "# Lets Forecast and Correlate Multi Sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c61f17c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#! pip install pycaret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1a4baab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pycaret.regression import *\n",
    "import pycaret, numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "396c8f3f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "037656ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "dro = pd.read_csv('~/Downloads/Drug Related Offenses 2.csv')\n",
    "# get narco events\n",
    "dro = dro.fillna(0)\n",
    "dro.groupby('Offense Type').count()\n",
    "narco = dro[dro['Offense Type'] == 'Drug/Narcotic Violations'][1:]\n",
    "# standard transforms\n",
    "narco['date'] = pd.to_datetime(narco['Incident Date'], errors='coerce')\n",
    "narco = narco.sort_values(by='date', ascending=True, ignore_index=True)\n",
    "narco['Number of Crimes'] = narco['Number of Crimes'].astype(int)\n",
    "#narco.set_index('date', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4310ad53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dateset(df, year_split, drop, keep):\n",
    "    data = df.copy()\n",
    "\n",
    "    data['Month'] = [i.month for i in data['date']]\n",
    "    data['Year'] = [i.year for i in data['date']]\n",
    "    # create a sequence of numbers\n",
    "    data['Series'] = np.arange(1,len(data)+1)\n",
    "    # drop unnecessary columns and re-arrange\n",
    "    data.drop(drop, axis=1, inplace=True)\n",
    "    data = data[keep] \n",
    "    # check the head of the dataset\n",
    "    #data.head()\n",
    "\n",
    "    train = data[data.Year<year_split]\n",
    "    test = data[data.Year>year_split]\n",
    "    return train, test, data\n",
    "\n",
    "def train_caret(df, year_split, drop, keep, target, numeric_features, session_id=123):\n",
    "    \n",
    "    train, test, data = get_dateset(df, year_split, drop, keep)\n",
    "    # initialize setup\n",
    "    s = setup(data = train, test_data = test, \n",
    "              target = target, \n",
    "              fold_strategy = 'timeseries', \n",
    "              numeric_features = numeric_features, \n",
    "              fold = 3, \n",
    "              transform_target = False, \n",
    "              session_id = session_id)\n",
    "\n",
    "    best = compare_models(sort = 'MAE')\n",
    "    prediction_holdout = predict_model(best)\n",
    "\n",
    "    predictions = predict_model(best, data=data)\n",
    "\n",
    "    predictions['date'] = df['date']\n",
    "    predictions[target] = df[target]\n",
    "\n",
    "    predictions.plot(x='date', y=[target, 'Label'], figsize=(15, 10))\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e3c3d61",
   "metadata": {},
   "outputs": [],
   "source": [
    "narco"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcfed640",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "drop = ['date', 'Offense Type', 'Incident Date']\n",
    "keep = ['Series', 'Year', 'Month', 'Number of Crimes']\n",
    "train, test = get_dateset(narco, 2017, drop, keep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2950d1ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = train_caret(narco, 2017, drop, keep, 'Number of Crimes', ['Series', 'Year', 'Month'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a23552d",
   "metadata": {},
   "outputs": [],
   "source": [
    "odf = pd.read_csv('~/Downloads/Opioid Report.csv') #skiprows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2b37b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "odf = odf[1:]\n",
    "odf = odf.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d0602d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "odf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a0cecbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "odf['Number of Drug Reports']=odf['Number of Drug Reports'].apply(lambda x: 0 if ',' in str(x) else int(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c70d5d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "odf['date'] = pd.to_datetime(odf['Incident Date'], errors='coerce')\n",
    "odf = odf.sort_values(by='date', ascending=True, ignore_index=True)\n",
    "odf['Number of Drug Reports'] = odf['Number of Drug Reports'].astype(int)\n",
    "odf = odf[:-5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1a55743",
   "metadata": {},
   "outputs": [],
   "source": [
    "drop = ['date', 'Drug Type', 'Incident Date']\n",
    "keep = ['Series', 'Year', 'Month', 'Number of Drug Reports']\n",
    "\n",
    "## need to add outlier threshold\n",
    "train, test, d = get_dateset(odf, 2017, drop, keep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9090c03e",
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_features = ['Year', 'Month', 'Series']\n",
    "target = 'Number of Drug Reports'\n",
    "# outliers don't do well here... would need to resample/prune, but scores are okay...\n",
    "preds = train_caret(odf, 2017, drop, keep, target, ['Series', 'Year', 'Month'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de3b2a47",
   "metadata": {},
   "outputs": [],
   "source": [
    "odf.resample('M', on='date').sum().plot(figsize=(15,7)) #compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5e60662",
   "metadata": {},
   "outputs": [],
   "source": [
    "hdf = pd.read_csv('~/Downloads/Heroin & Fentanyl.csv', skiprows=30, names='date place drug counts'.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cb9f023",
   "metadata": {},
   "outputs": [],
   "source": [
    "hdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe476706",
   "metadata": {},
   "outputs": [],
   "source": [
    "hdf['counts'] = hdf['counts'].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e26cb9e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "hdf['date'] = pd.to_datetime(hdf['date'], errors='coerce')\n",
    "hdf = hdf.sort_values(by='date', ascending=True, ignore_index=True)\n",
    "hdf['counts'] = hdf['counts'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58254aaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "drug_counts = hdf.groupby('drug').resample('W', on='date').sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "950a49be",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "hdf.resample('3M', on='date').sum().plot(figsize=(15,7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c19fe230",
   "metadata": {},
   "outputs": [],
   "source": [
    "hhdf = hdf[hdf.drug=='Heroin']\n",
    "hhdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "793ecff8",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "numeric_features = ['Year', 'Month', 'Series']\n",
    "target = 'counts'\n",
    "drop = ['date', 'place', 'drug']\n",
    "keep = ['Year', 'Month', 'Series', 'counts']\n",
    "\n",
    "#detects definite trend jumps year over year\n",
    "preds = train_caret(hhdf, 2017, drop, keep, target, ['Series', 'Year', 'Month'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cf21409",
   "metadata": {},
   "source": [
    "## Compare datasets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecc7b154",
   "metadata": {},
   "outputs": [],
   "source": [
    "odf.groupby('Drug Type').sum() #all Drug Types counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11316118",
   "metadata": {},
   "outputs": [],
   "source": [
    "hodf = odf[odf['Drug Type'] == 'Heroin']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26fd184c",
   "metadata": {},
   "outputs": [],
   "source": [
    "hodf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cef37f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(hodf), len(hhdf) # both datasets are similar size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4e03336",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "res = pd.merge_asof(hodf, hhdf, on='date') #cool function\n",
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "394e6cc5",
   "metadata": {},
   "source": [
    "## Now we can compare heroin correlation between two datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "167f02a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "stats.pearsonr(res['Number of Drug Reports'], res.counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d90566d",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.corrcoef(res['Number of Drug Reports'], res.counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f492972",
   "metadata": {},
   "outputs": [],
   "source": [
    "# so the heroin - heroin cor is high between the two datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0cfd06a",
   "metadata": {},
   "source": [
    "# Get news "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3b8b819",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datanews\n",
    "from dateutil import parser\n",
    "from pprint import pprint\n",
    "\n",
    "def parse_datestring(datestring):\n",
    "    # turns '2021-08-24T21:26:08+00:00' into 'Tue Aug 24 21:26:08 2021'\n",
    "    return parser.parse(datestring)\n",
    "\n",
    "def get_unique_hits(docs):\n",
    "    untitles = set([k[\"title\"] for k in docs])\n",
    "    ndocs = []\n",
    "    for k in docs:\n",
    "        if k[\"title\"] in untitles:\n",
    "            ndocs.append(k)\n",
    "            untitles.remove(k[\"title\"])\n",
    "    return ndocs\n",
    "\n",
    "def get_news(query, from_date=None, to_date=None, source=None, language=\"en\"):\n",
    "    # from_date='2021-11-11'\n",
    "    datanews.api_key = \"0lhxl30stv3dfd0jo8yz1pghm\"\n",
    "    response = datanews.news(\n",
    "        q=query,\n",
    "        from_date=from_date,\n",
    "        to_date=to_date,\n",
    "        language=language,\n",
    "        source=source,\n",
    "        size=100,\n",
    "    )\n",
    "    if 'hits' in response:\n",
    "        articles = response[\"hits\"]\n",
    "        pprint(Counter([row[\"title\"] for row in articles]).most_common(10))\n",
    "        for k in articles:\n",
    "            k[\"pubDate\"] = parse_datestring(k[\"pubDate\"])\n",
    "        return get_unique_hits(articles)\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d1ab1a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "for query in ['opiod overdose in tennessee', 'opiod deaths in tennessee', \\\n",
    "              'heroine deaths in tennessee', 'heroine overdoses in tennessee', \\\n",
    "             'illegal drugs in tennessee', 'drug busts in tennessee', 'drug rings in tennessee']:\n",
    "    articles = get_news(query, from_date='2021-01-01', to_date='2022-02-16')\n",
    "    data.append([query, articles])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee40910c",
   "metadata": {},
   "outputs": [],
   "source": [
    "[len(k[1]) for k in data ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca8baad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da85accc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tennessee_news(data):\n",
    "    tdata = []\n",
    "    i = 0\n",
    "    for q, resses in data:\n",
    "        for row in resses:\n",
    "            if ('tennessee' in row['title']) or ('tennessee' in row['description']) or ('tennessee' in row['source']):\n",
    "                tdata.append(row)\n",
    "                i+=1\n",
    "                print(f'{i}')\n",
    "    return tdata\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "663aaa4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "news = get_tennessee_news(data) #not many"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1b14559",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5d40f54",
   "metadata": {},
   "outputs": [],
   "source": [
    "allnews = [[l['title'], l['content'].split('...')[0]] for k in data for l in k[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86818ddb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "allnews"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4ef5028",
   "metadata": {},
   "source": [
    "# however, the featurization and umap gives good clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbadc817",
   "metadata": {},
   "outputs": [],
   "source": [
    "aln = pd.DataFrame(allnews, columns = ['title', 'content'])\n",
    "aln['n'] = range(len(aln))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51343b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "g5 = g.nodes(aln, 'n').featurize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9532d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "g6 = g5.umap(scale=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2943eebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "g6.plot()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
