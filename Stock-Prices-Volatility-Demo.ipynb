{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2f634c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install yfinance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96ab3909",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import yfinance as yf\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import graphistry\n",
    "from graphistry.feature_utils import get_dtypes_for_dataframe\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90b91d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "graphistry.register(api=3, protocol=\"https\", server=\"hub.graphistry.com\", username=\"\", password=\"\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "797ce313",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = graphistry.bind() # create base graphistry instance that we can reuse below"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e49c69e",
   "metadata": {},
   "source": [
    "# Load in 100 leading industry Tickers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "807f7890",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = pd.read_html('https://en.wikipedia.org/wiki/S%26P_100')\n",
    "ndf = res[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "366c9d51",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# load bulk\n",
    "ndf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "040320fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "tickers = ndf[ndf.Symbol != 'BRK.B'].Symbol # since BRK.B is delisted."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73016c00",
   "metadata": {},
   "source": [
    "# We add in 33 Electric Car Stocks from different parts of the supply chain \n",
    "## -- Manufacturers, Battery Systems, Material Stocks, Charging Stations and Equipement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25e014a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get Electric Car Stocks + supply chain heuristics\n",
    "ev_tickers = ['GM', 'F', 'XPEV', 'LI', 'NIO', 'TSLA', 'NKLA', 'RIVN', 'RIDE', 'TM',\n",
    "              'GOEV', 'FSR', 'APTV', 'FUV', 'BLNK', 'LCID', 'CHPT', 'GELYF', 'MGA', 'DDAIF',\n",
    "             'PCRFY', 'VWAGY', 'BMWYY', 'ZEV', 'SLDP', 'RMO', 'PCAR', 'LAC', #battery systems\n",
    "             'NNDM', 'BWA', 'MP', 'FCX', 'ALSN', 'ALB',     #Materials Stocks\n",
    "             'BEEM', 'BLNK', 'VLTA' # Charging stations + equipment\n",
    "             ] # add your own here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44c4db68",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tickers = tickers.to_list()+ev_tickers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01b4ff47",
   "metadata": {},
   "outputs": [],
   "source": [
    "fetch = True\n",
    "if fetch:\n",
    "    # download their price data\n",
    "    df_prices_all = yf.download(all_tickers, start='2021-01-01', interval='1h')\n",
    "    df_prices_all.to_csv('data/stock_prices_all.csv')\n",
    "else:\n",
    "    df_prices_all = pd.read_csv('data/stock_prices_all.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "052f9a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_prices_all"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24ad3475",
   "metadata": {},
   "source": [
    " This is a multi-Index DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0505259",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_prices_all.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b655d706",
   "metadata": {},
   "outputs": [],
   "source": [
    "def enrich_from_tickers(tickers):\n",
    "    \"\"\"\n",
    "        Get's enrichment data per ticker.\n",
    "        Runs slow -- several minutes for 133 tickers.\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    for tick in tickers:\n",
    "        print(f'Downloading info for {tick}')\n",
    "        res = yf.Ticker(tick)\n",
    "        datum = res.info\n",
    "        data.append(datum)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2a5c855",
   "metadata": {},
   "outputs": [],
   "source": [
    "fetch = False\n",
    "if fetch:\n",
    "    data = enrich_from_tickers(all_tic)\n",
    "    df  = pd.DataFrame(data)\n",
    "    df.to_csv('data/stocks_metadata.csv')\n",
    "else:\n",
    "    df = pd.read_csv('data/stocks_metadata.csv', index_col=0)\n",
    "    df['n'] = range(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8a08562",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we have many examples of sector\n",
    "Counter(df.sector).most_common(), df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0a1c34a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## see what we got\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "834c4602",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get some volatility metrics\n",
    "o = df_prices_all.Open\n",
    "h = df_prices_all.High\n",
    "l = df_prices_all.Low\n",
    "c = df_prices_all.Close\n",
    "\n",
    "# square of GARMAN-KLASS rv estimator from OLHC data\n",
    "vol = 1/2 * np.square(np.log(h/l)) - (2*np.log(2)-1)*np.square(np.log(c/o))\n",
    "vol = vol.replace([np.inf, np.nan], 0)\n",
    "\n",
    "# square of Rogers-Satchell Volatility \n",
    "vol2 = np.log(h/c)*np.log(h/o)+ np.log(l/c)*np.log(l/o)\n",
    "vol2 = vol2.replace([np.inf, np.nan], 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fb78a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "good_tickers = vol.columns[vol.columns!='BRK.B']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "145ef2e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "vol = vol[good_tickers]\n",
    "vol2 = vol2[good_tickers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f93c64c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# have to add in a node identifier, so we use add\n",
    "vol.T['n'] = range(len(vol.T)) # FIXME this breaks featurizer, why?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d87246b",
   "metadata": {},
   "source": [
    "# Now we can UMAP Volitility Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c1d1c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rogers-Satchell Volatility \n",
    "g2 = g.nodes(vol2.T)\n",
    "g3 = g2.umap(scale = 2, n_neighbors=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc188812",
   "metadata": {},
   "outputs": [],
   "source": [
    "# g2 = g.nodes(vol.T, 'n').featurize()\n",
    "# g3 = g2.umap(scale = 2, n_neighbors=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8585ea43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# UMAP has created an implicit Edge DataFrame\n",
    "g3.weighted_edges_df_from_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7fa9b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "## this will cluster groups according to Rogers-Satchell Volatility\n",
    "# Shows how easy it is to cluster by a given metric of interest!\n",
    "\n",
    "g4 = g.nodes(df, 'n').edges(g3._edges, '_src', '_dst')\n",
    "g4.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c790eb1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#vol.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d553fdd5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#vol2.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb16b362",
   "metadata": {},
   "source": [
    "# Let's see if we can cluster by  Adj Close value over time. We will see that this naturally clusters by industry category in a semantically useful way. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fda08685",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_prices = df_prices_all['Adj Close']\n",
    "df_prices = df_prices[good_tickers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07e69bff",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5637087f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we have to run some cleanup on NaN values, so we use Imputer and MinMax Scaling\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "colnames = df_prices.columns\n",
    "dates = df_prices.index\n",
    "\n",
    "# impute values \n",
    "imputer = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "imputer = imputer.fit(df_prices)\n",
    "res = imputer.transform(df_prices)\n",
    "df_prices = pd.DataFrame(res, index=df_prices.index, columns = df_prices.columns)\n",
    "\n",
    "# scale the resulting values\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "df_prices = pd.DataFrame(scaler.fit_transform(df_prices), index=dates, columns=colnames)\n",
    "df_prices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdae18a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_adj = df_prices.T\n",
    "df_adj['n'] = range(len(df_adj))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0acaceaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_adj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55548f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "g2 = g.nodes(df_adj, 'n').featurize()\n",
    "g3 = g3.umap(scale=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eafde38",
   "metadata": {},
   "outputs": [],
   "source": [
    "g3.plot()  # Why is this happening???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbbc41d4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "g4 = g.nodes(df, 'n').edges(g3._edges, '_src', '_dst')\n",
    "g4.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8c3d4bb",
   "metadata": {},
   "source": [
    "# Can we use the data from DF (rather than price flows) to cluster by sector? We will see that this does a great job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b262b9b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# we want to separate data into Numeric and not, \n",
    "gtypes = get_dtypes_for_dataframe(df, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58c08644",
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_df = df[gtypes['float64'] + gtypes['int64']]\n",
    "numeric_df = numeric_df.replace([np.inf, np.nan], 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e4552a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65de01b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "g2 = g.nodes(numeric_df, 'n').featurize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8228ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#g2 = g.nodes(numeric_df, 'symbol').featurize()\n",
    "g3 = g2.umap(scale=.5, n_neighbors=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "072c8442",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "g3.plot() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8de158e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# or seeing it with the better label (company name)\n",
    "g4 = graphistry.nodes(df, 'n').edges(g3.weighted_edges_df_from_nodes, '_src','_dst')\n",
    "g4.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e681691",
   "metadata": {},
   "source": [
    "# A few amazing observations -- It has clustered, just from financial data, the EV companies by sector. \n",
    "\n",
    "It finds all the major EV categories we started with, and even some suprising relationships between correlated stock data. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88359575",
   "metadata": {},
   "source": [
    "## Next we use Textual and Category data and see how well it clusters (sans finance data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd68ba70",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_cols = ['n','longBusinessSummary', 'industry', 'sector', 'financialCurrency', 'longName']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2fd44e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "meta = df[summary_cols]\n",
    "meta = meta.fillna('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b81d4a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "g2 = g.nodes(meta, 'n').featurize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6e3aa09",
   "metadata": {},
   "outputs": [],
   "source": [
    "g3 = g2.umap(scale=0.5, n_neighbors=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aa7989c",
   "metadata": {},
   "source": [
    "## We see the similarity from textual and other categorical metadata, and it does quite well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a0ca800",
   "metadata": {},
   "outputs": [],
   "source": [
    "g3.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03e9e315",
   "metadata": {},
   "source": [
    "# Some stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6da3d72f",
   "metadata": {},
   "outputs": [],
   "source": [
    "weekly_std = df_prices.resample('W', on='timestamp').std() #.agg(['mean', 'min', 'max', 'std', 'skew'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "933f5a7b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "weekly_std.plot(figsize=(17,10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01dced80",
   "metadata": {},
   "source": [
    "# Can we run on the volitility data again using a trick to make new features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92cbd75f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_target = vol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dd2f81b",
   "metadata": {},
   "outputs": [],
   "source": [
    "mdf = df_target.resample('W').agg(['mean', 'min', 'max', 'std', 'skew']).T\n",
    "mdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d30a6e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "g2 = g.nodes(mdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "887e18a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "g3 = g2.umap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8c644d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "g3.weighted_edges_df_from_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fcd31e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "mdf_node_name = np.array(list(df.symbol.values)*5).reshape(5, len(df)).T.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c71ea1d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "hdf = pd.DataFrame({'symbol': list(mdf_node_name)})\n",
    "hdf['n'] = range(len(mdf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38b0c9e4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "g4 = graphistry.nodes(hdf, 'n').edges(g3.weighted_edges_df_from_nodes, '_src','_dst')\n",
    "g4.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d886d051",
   "metadata": {},
   "source": [
    "# Lastly Let's compare clustering to a useful PLSR.B regressor, called CCA (Canonical Correlation Analyis)\n",
    "## We will see that it doesn't do as well as Featurization + UMAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed06342b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cross_decomposition import CCA\n",
    "clf = CCA(n_components=2)\n",
    "res=clf.fit_transform(df_prices, df_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27147d09",
   "metadata": {},
   "outputs": [],
   "source": [
    "res[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "227a15b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# two sets of components plotted over time\n",
    "lf = pd.DataFrame({'comp1_a':res[0].T[0], \n",
    "                   'comp2_a':res[0].T[1], \n",
    "                   'comp1_b':res[1].T[0], 'comp2_b':res[1].T[1]}, index=df_target.index)\n",
    "lf.plot(figsize=(15, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ac2ef68",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.imshow(clf.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5976bd37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can use the coef_ of clf to make an edge dataframe\n",
    "a, b = clf.coef_.nonzero()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b450d504",
   "metadata": {},
   "outputs": [],
   "source": [
    "CCA_edges = pd.DataFrame({'_src':a, '_dst':b, 'weight': clf.coef_[a,b]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4a3fd56",
   "metadata": {},
   "outputs": [],
   "source": [
    "from graphistry.feature_utils import prune_weighted_edges_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cba9fe1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "wdf = prune_weighted_edges_df(CCA_edges, scale=3) # higher scale to reduce connectivity (big blob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e728cbcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "g2 = g.nodes(meta, 'n').edges(wdf, '_src', '_dst')   \n",
    "g2.plot()  # lots of standalone nodes..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ce3f6ac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
